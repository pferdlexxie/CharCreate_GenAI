{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T13:39:12.375960Z",
     "iopub.status.busy": "2025-03-10T13:39:12.375672Z",
     "iopub.status.idle": "2025-03-10T13:39:19.480872Z",
     "shell.execute_reply": "2025-03-10T13:39:19.479914Z",
     "shell.execute_reply.started": "2025-03-10T13:39:12.375930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors peft bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code fine-tunes the Stable Diffusion model using LoRA with a custom Pinterest dataset. The dataset, consisting of images in PNG format, is used to adapt the model to a specific style or theme related to Pinterest. \n",
    "\n",
    "#### The process includes:\n",
    "##### Loading Pretrained Model: Stable Diffusion model is loaded from runwayml/stable-diffusion-v1-5.\n",
    "##### Dataset Preparation: Images are resized to 512x512 and converted to tensors.\n",
    "##### Fine-Tuning with LoRA: The UNet model is fine-tuned using the LoRA technique, optimizing model weights while keeping resource usage low.\n",
    "##### Training Loop: The model is trained on the dataset by generating noisy images, applying the diffusion process, and minimizing the loss between predicted and actual noise.\n",
    "##### Saving the LoRA Adapter: After training, the LoRA adapter is saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T15:03:34.755036Z",
     "iopub.status.busy": "2025-03-06T15:03:34.754703Z",
     "iopub.status.idle": "2025-03-06T17:22:48.009836Z",
     "shell.execute_reply": "2025-03-06T17:22:48.008992Z",
     "shell.execute_reply.started": "2025-03-06T15:03:34.755008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b1243193a940d2a0d83b86141c2587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8886cb512a4c3287f98a08f8875e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb09771294d4c38948cc902ef861c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8c93ef650f4823b5eb52f269f30723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38eecb74a384252ac06e6ad6a8d6593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28ee332ccfe4a97b9653213e14adf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c437202f56f84184950838a54d416d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa3701d765349f4b850fbce8750db72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7453326ec4024fd592af5d7e4571cdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff8c7ae890e4c2db796c349099c3892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c868202d221049469158fbaac330aecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a667743e480949e7a24e643265cbd9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fd46d77e854b3597a6aa53bb3e4290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c30924706364869aa7569e0cf05bec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f17226975004e7b931353c59c8ab677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da27aed0fef542a7adffe3403047eece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80a363439be4fe696eb3dba54444bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc355db254594d64a405db4c2c5b8870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,594,368 || all params: 861,115,332 || trainable%: 0.1852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:50<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:49<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.59033203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:49<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.79638671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:50<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.82763671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:50<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.7568359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:50<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.80126953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:50<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.76220703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:50<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.78466796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:50<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.73583984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [13:50<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.865234375\n",
      "LoRA fine-tuning complete! Adapter saved to lora_finetuned_sd15\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from diffusers import AutoencoderKL\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "dataset_path = \"/kaggle/input/hpdataset/HPdataset\"\n",
    "output_dir = \"lora_finetuned_sd15\"\n",
    "\n",
    "# Load base model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)  # Load the pretrained model\n",
    "unet = pipe.unet.to(\"cuda\").half()  # UNet model for denoising, moved to GPU and set to float16\n",
    "vae = pipe.vae.to(\"cuda\").half()  # VAE for encoding images, moved to GPU and set to float16\n",
    "text_encoder = pipe.text_encoder.to(\"cuda\").half()  # Text encoder, moved to GPU and set to float16\n",
    "\n",
    "# LoRA configuration setup\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank for the LoRA layers\n",
    "    lora_alpha=16,  # Scaling factor for LoRA\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],  # LoRA applied to these linear layers\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    bias=\"none\"  # No bias in LoRA layers\n",
    ")\n",
    "\n",
    "# Apply LoRA to the UNet model\n",
    "unet = get_peft_model(unet, lora_config)  # Wrap UNet with LoRA layers\n",
    "unet.print_trainable_parameters()  # Print parameters to verify trainable layers\n",
    "\n",
    "# Load custom dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder):\n",
    "        # Initialize dataset by loading all PNG images\n",
    "        self.image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert images to Tensor (range [0, 1])\n",
    "            transforms.Resize((512, 512)),  # Resize images to 512x512\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)  # Return total number of images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and transform image for a specific index\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")  # Convert to RGB\n",
    "        image = self.transform(image)  # Apply transformations\n",
    "        return image\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CustomDataset(dataset_path)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Training loop setup\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)  # AdamW optimizer for training\n",
    "epochs = 10  # Number of epochs for training\n",
    "max_timesteps = 1000  # Maximum timesteps for the diffusion process\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = batch.to(\"cuda\").half()  # Move data to GPU and set to float16\n",
    "\n",
    "        # Encode image using VAE to obtain latent representation\n",
    "        with torch.no_grad():\n",
    "            batch_latents = vae.encode(batch).latent_dist.sample().detach() * 0.18215  # Scale latents appropriately\n",
    "\n",
    "        # Generate random timesteps for diffusion process\n",
    "        timesteps = torch.randint(0, max_timesteps, (batch.size(0),), device=batch.device, dtype=torch.int64)\n",
    "        \n",
    "        # Generate random noise for the image\n",
    "        noise = torch.randn_like(batch_latents, dtype=torch.float16)\n",
    "        noisy_latents = batch_latents + 0.1 * noise  # Add noise to latents\n",
    "\n",
    "        # Dummy text input to simulate text encoder processing\n",
    "        dummy_input_ids = torch.randint(0, 1000, (batch.size(0), 77), device=batch.device)  # Random text tokens\n",
    "        encoder_hidden_states = text_encoder(dummy_input_ids).last_hidden_state  # Encoder output\n",
    "\n",
    "        # Forward pass through UNet model (denoising)\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample  # Generate denoised latents\n",
    "\n",
    "        # Compute loss (MSE loss between predicted and actual noise)\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the LoRA-adapted UNet model\n",
    "unet.save_pretrained(output_dir)\n",
    "print(\"LoRA fine-tuning complete. Adapter saved to\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6805095,
     "sourceId": 10942070,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
