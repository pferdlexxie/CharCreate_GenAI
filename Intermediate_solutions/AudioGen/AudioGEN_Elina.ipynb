{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Survey: Evaluating AI-Generated Music Based on Emotions**  \n",
    "\n",
    "1. **What mood would you like to hear in the music?**\n",
    "   - Joyful  \n",
    "   - Calm  \n",
    "   - Energetic  \n",
    "   - Sad  \n",
    "   - Dreamy  \n",
    "   - Aggressive  \n",
    "\n",
    "2. **In what context would you listen to this music?**  \n",
    "   - Walking  \n",
    "   - Background for work  \n",
    "   - Sports / Activity  \n",
    "   - Relaxation / Meditation  \n",
    "   - Party\n",
    "\n",
    "3. **Do you have any preferred music style?**  \n",
    "   - Electronic  \n",
    "   - Classical  \n",
    "   - Rock  \n",
    "   - Jazz  \n",
    "   - Light instrumental  \n",
    "   - No preference\n",
    "\n",
    "4. **Which instruments do you like?**\n",
    "   - Guitar  \n",
    "   - Piano  \n",
    "   - Violin / Strings  \n",
    "   - Drums  \n",
    "   - Brass / Wind instruments  \n",
    "   - Synthesizers / Electronic sounds\n",
    "\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "\n",
    "startpath = '.'\n",
    "list_files(startpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "AUDIO_DIR = \"DEAM/DEAM_audio/processed/\"\n",
    "OUTPUT_DIR = \"DEAM/DEAM_audio/processed_normalized/\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Get the list of all audio files in the processed folder\n",
    "audio_files = [f for f in os.listdir(AUDIO_DIR) if f.endswith(\".mp3\")]\n",
    "\n",
    "# Function to normalize audio files\n",
    "def normalize_audio(file_path, output_path):\n",
    "    audio = AudioSegment.from_mp3(file_path)\n",
    "    \n",
    "    normalized_audio = audio.normalize()\n",
    "    \n",
    "    normalized_audio.export(output_path, format=\"mp3\")\n",
    "    \n",
    "    print(f\"Normalized and saved file: {output_path}\")\n",
    "\n",
    "for file in audio_files:\n",
    "    file_path = os.path.join(AUDIO_DIR, file)\n",
    "    output_path = os.path.join(OUTPUT_DIR, file)\n",
    "    normalize_audio(file_path, output_path)\n",
    "\n",
    "print(\"Normalization of all files is complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode_survey_response(survey_response):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bert_model.to(device)\n",
    "    \n",
    "    if isinstance(survey_response, torch.Tensor):  \n",
    "        survey_response = survey_response.tolist()\n",
    "    \n",
    "    if isinstance(survey_response, list) and all(isinstance(x, int) for x in survey_response):\n",
    "        survey_response = tokenizer.decode(survey_response)\n",
    "    \n",
    "    input_text = survey_response if isinstance(survey_response, str) else \" \".join(map(str, survey_response))\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state[:, 0, :].cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "def load_audio_features(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def extract_audio_features(audio_file_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    \n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
    "    return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MusicGenerator, self).__init__()\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, nhead=8, num_encoder_layers=6)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, user_embedding, audio_features):\n",
    "        # Combine user embedding and audio features\n",
    "        x = torch.cat((user_embedding, audio_features), dim=1)\n",
    "        \n",
    "        # Pass through the network\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.transformer(x, x)\n",
    "        output = self.fc2(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def extract_audio_features(audio_file_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    \n",
    "    # Extracting Mel Spectrogram as an example of features\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
    "    \n",
    "    # Normalize the Mel Spectrogram\n",
    "    mel_spectrogram = F.pad(mel_spectrogram, (0, 1), mode='constant', value=0)\n",
    "    return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class MusicSurveyDataset(Dataset):\n",
    "    def __init__(self, survey_responses, audio_files):\n",
    "        self.survey_responses = survey_responses\n",
    "        self.audio_files = audio_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.survey_responses)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the survey responses\n",
    "        survey_response = self.survey_responses[idx]\n",
    "        \n",
    "        # Load audio features from CSV\n",
    "        audio_features = extract_audio_features(self.audio_files[idx])\n",
    "        \n",
    "        # Get BERT embeddings for the survey response\n",
    "        user_embedding = encode_survey_response(survey_response)\n",
    "        \n",
    "        return user_embedding, audio_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example survey responses and corresponding audio feature CSV paths\n",
    "survey_responses = [\"Energetic\", \"Calm\", \"Sad\"]  \n",
    "\n",
    "audio_files = [\n",
    "    \"DEAM/DEAM_audio/processed_normalized/10.mp3\", \n",
    "    \"DEAM/DEAM_audio/processed_normalized/1000.mp3\", \n",
    "    \"DEAM/DEAM_audio/processed_normalized/1001.mp3\"\n",
    "]\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "dataset = MusicSurveyDataset(survey_responses, audio_files)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now I have some problems with training model, but I'm working on this\n",
    "def train(model, data_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for user_response, audio_features in data_loader:\n",
    "            # Get user embedding from survey response\n",
    "            user_embedding = encode_survey_response(user_response)\n",
    "            \n",
    "            # Pass through the model\n",
    "            generated_features = model(user_embedding, audio_features)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(generated_features, audio_features)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Model, optimizer, and criterion\n",
    "input_size = 768\n",
    "hidden_size = 512\n",
    "output_size = 256\n",
    "model = MusicGenerator(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "train(model, data_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, survey_response):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode user input\n",
    "    user_embedding = encode_survey_response(survey_response)\n",
    "    \n",
    "    # Generate music features\n",
    "    generated_features = model(user_embedding, torch.zeros_like(user_embedding))\n",
    "    \n",
    "    # Convert the generated features into audio waveform\n",
    "    generated_audio = features_to_audio(generated_features)\n",
    "    return generated_audio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work:\n",
    "Once we have generated the audio features, we’ll need to convert them back into an audio signal (like a waveform). This is a non-trivial task and requires a method to map the features back into an audio signal, such as using WaveNet or another generative model designed for audio synthesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
